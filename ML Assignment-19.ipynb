{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa470015",
   "metadata": {},
   "source": [
    "1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2 and that the first set of random centroid is 15, 32, and that the second set is 12, 30.\n",
    "\n",
    "a) Using the k-means method, create two clusters for each set of centroid described above.\n",
    "\n",
    "b) For each set of centroid values, calculate the SSE.\n",
    "\n",
    "2. Describe how the Market Basket Research makes use of association analysis concepts.\n",
    "\n",
    "3. Give an example of the Apriori algorithm for learning association rules.\n",
    "\n",
    "4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metricis used to decide when to end the iteration.\n",
    "\n",
    "5. In the k-means algorithm, how do you recompute the cluster centroids?\n",
    "\n",
    "6. At the start of the clustering exercise, discuss one method for determining the required number ofclusters.\n",
    "\n",
    "7. Discuss the k-means algorithm&#39;s advantages and disadvantages.\n",
    "\n",
    "8. Draw a diagram to demonstrate the principle of clustering.\n",
    "\n",
    "9. During your study, you discovered seven findings, which are listed in the data points below. Usingthe K-means algorithm, you want to build three clusters from these observations.The clusters C1,C2, and C3 have the following findings after the first iteration:\n",
    "\n",
    "C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),\n",
    "\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
    "\n",
    "C3: (5,5) and (9,9)\n",
    "\n",
    "What would the cluster centroids be if you were to run a second iteration? What would this\n",
    "clustering&#39;s SSE be?\n",
    "\n",
    "10. In a software project, the team is attempting to determine if software flaws discovered duringtesting are identical. Based on the text analytics of the defect details, they decided to build 5 clustersof related defects. Any new defect formed after the 5 clusters of defects have been identified mustbe listed as one of the forms identified by clustering. A simple diagram can beused to explain thisprocess. Assume you have 20 defect data points that are clustered into 5 clusters and you used the k-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16ab77",
   "metadata": {},
   "source": [
    "# Solution:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ecbb69",
   "metadata": {},
   "source": [
    "1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2 and that the first set of random centroid is 15, 32, and that the second set is 12, 30.\n",
    "\n",
    "a) Using the k-means method, create two clusters for each set of centroid described above.\n",
    "To create clusters using the k-means method, we follow these steps:\n",
    "\n",
    "Set the initial centroids:\n",
    "\n",
    "For the first set of centroids (15, 32), the initial cluster assignments are: Cluster 1: [], Cluster 2: []\n",
    "For the second set of centroids (12, 30), the initial cluster assignments are: Cluster 1: [], Cluster 2: []\n",
    "Assign data points to clusters based on the nearest centroid:\n",
    "\n",
    "For the first set of centroids (15, 32):\n",
    "Data point 5 is closer to centroid 15, so it is assigned to Cluster 1.\n",
    "Data points 10 and 15 are closer to centroid 15, so they are assigned to Cluster 1.\n",
    "Data point 20 is closer to centroid 15, so it is assigned to Cluster 1.\n",
    "Data points 25 and 30 are closer to centroid 32, so they are assigned to Cluster 2.\n",
    "Data point 35 is closer to centroid 32, so it is assigned to Cluster 2.\n",
    "For the second set of centroids (12, 30):\n",
    "Data point 5 is closer to centroid 12, so it is assigned to Cluster 1.\n",
    "Data points 10 and 15 are closer to centroid 12, so they are assigned to Cluster 1.\n",
    "Data point 20 is closer to centroid 30, so it is assigned to Cluster 2.\n",
    "Data points 25, 30, and 35 are closer to centroid 30, so they are assigned to Cluster 2.\n",
    "Recompute the cluster centroids:\n",
    "\n",
    "For the first set of centroids (15, 32), the new centroids are: Centroid 1: 10, Centroid 2: 30.\n",
    "For the second set of centroids (12, 30), the new centroids are: Centroid 1: 10, Centroid 2: 30.\n",
    "Repeat the assignment and centroid recomputation steps until convergence.\n",
    "\n",
    "b) For each set of centroid values, calculate the SSE (Sum of Squared Errors):\n",
    "\n",
    "For the first set of centroids (15, 32):\n",
    "SSE = (5-10)^2 + (10-10)^2 + (15-10)^2 + (20-30)^2 + (25-30)^2 + (30-30)^2 + (35-30)^2\n",
    "= 25 + 0 + 25 + 100 + 25 + 0 + 25\n",
    "= 200\n",
    "For the second set of centroids (12, 30):\n",
    "SSE = (5-10)^2 + (10-10)^2 + (15-10)^2 + (20-30)^2 + (25-30)^2 + (30-30)^2 + (35-30)^2\n",
    "= 25 + 0 + 25 + 100 + 25 + 0 + 25\n",
    "= 200\n",
    "\n",
    "2. Describe how Market Basket Research makes use of association analysis concepts.\n",
    "\n",
    "Market Basket Research uses association analysis concepts to discover relationships between items frequently purchased together by customers. It aims to identify patterns and associations within transaction data, typically from retail or e-commerce settings. The analysis focuses on finding co-occurrence relationships among items and generating association rules that can be used for various purposes such as product placement, cross-selling, and recommendation systems.\n",
    "\n",
    "3. Give an example of the Apriori algorithm for learning association rules.\n",
    "\n",
    "Let's consider a simple example where we have a transaction dataset containing customer purchases at a grocery store. The dataset consists of transactions with items purchased by customers. The Apriori algorithm can be used to learn association rules from this data.\n",
    "Suppose we want to find associations between items A and B. The Apriori algorithm follows these steps:\n",
    "\n",
    "Step 1: Generate frequent itemsets:\n",
    "\n",
    "Find all individual items that meet the minimum support threshold.\n",
    "In our case, we find all items that are purchased frequently enough, let's say items {A, B, C, D}.\n",
    "Step 2: Generate candidate itemsets:\n",
    "\n",
    "Create candidate itemsets by joining the frequent itemsets found in the previous step.\n",
    "In our case, we generate candidate itemsets {AB, AC, AD, BC, BD, CD}.\n",
    "Step 3: Prune infrequent itemsets:\n",
    "\n",
    "Remove candidate itemsets that do not meet the minimum support threshold.\n",
    "In our case, let's assume AB, AC, and BD do not meet the minimum support threshold and are pruned.\n",
    "Step 4: Generate association rules:\n",
    "\n",
    "For each remaining frequent itemset, generate association rules by splitting the itemset into antecedent and consequent.\n",
    "In our case, we generate association rules:\n",
    "A -> B (with support and confidence metrics)\n",
    "B -> A (with support and confidence metrics)\n",
    "C -> D (with support and confidence metrics)\n",
    "D -> C (with support and confidence metrics)\n",
    "\n",
    "4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric is used to decide when to end the iteration.\n",
    "\n",
    "In hierarchical clustering, the distance between clusters is typically measured using methods such as single linkage, complete linkage, or average linkage. These methods calculate the distance between clusters based on the distances between individual data points within each cluster.\n",
    "\n",
    "Single linkage (or minimum linkage) measures the distance between two clusters as the minimum distance between any two points in the clusters.\n",
    "\n",
    "Complete linkage (or maximum linkage) measures the distance between two clusters as the maximum distance between any two points in the clusters.\n",
    "\n",
    "Average linkage measures the distance between two clusters as the average distance between all pairs of points from the two clusters.\n",
    "\n",
    "The iteration in hierarchical clustering continues until all data points are assigned to a single cluster or until a specific stopping criterion is met. This stopping criterion can be based on a predetermined number of clusters or a threshold distance value. For example, if the distance between clusters exceeds a certain threshold, the iteration stops, and the clusters formed at that point are considered the final result.\n",
    "\n",
    "5. In the k-means algorithm, how do you recompute the cluster centroids?\n",
    "\n",
    "In the k-means algorithm, the cluster centroids are recomputed in each iteration based on the mean position of data points assigned to each cluster. The steps to recompute the cluster centroids are as follows:\n",
    "\n",
    "Assign each data point to its nearest centroid based on distance (usually Euclidean distance).\n",
    "\n",
    "Once all data points are assigned to clusters, compute the mean position (centroid) for each cluster by taking the average of the positions of the data points within that cluster.\n",
    "\n",
    "Update the centroid positions to the newly computed mean positions.\n",
    "\n",
    "Repeat the assignment and centroid update steps until convergence is reached (i.e., when the centroids stabilize and do not change significantly between iterations).\n",
    "\n",
    "6. At the start of the clustering exercise, discuss one method for determining the required number of clusters.\n",
    "\n",
    "One method for determining the required number of clusters at the start of a clustering exercise is the Elbow Method. The Elbow Method involves plotting the variance (or sum of squared distances) within clusters as a function of the number of clusters. The goal is to identify the number of clusters where adding more clusters does not significantly reduce the variance.\n",
    "\n",
    "The Elbow Method suggests that the number of clusters should be chosen at the \"elbow\" point on the plot, where the rate of variance reduction begins to level off. This point indicates a balance between capturing meaningful structure within the data and avoiding overfitting.\n",
    "\n",
    "7. Discuss the k-means algorithm's advantages and disadvantages.\n",
    "\n",
    "Advantages of the k-means algorithm:\n",
    "\n",
    "Simple and easy to implement.\n",
    "Fast and efficient for large datasets.\n",
    "Works well with spherical-shaped clusters.\n",
    "Can handle a large number of variables/features.\n",
    "Disadvantages of the k-means algorithm:\n",
    "\n",
    "Requires the number of clusters (k) to be specified in advance.\n",
    "Sensitive to the initial placement of centroids, which can lead to different results.\n",
    "Does not handle well with clusters of different sizes or non-linearly separable clusters.\n",
    "Can be sensitive to outliers, which can affect centroid positions and cluster assignments.\n",
    "\n",
    "8. Draw a diagram to demonstrate the principle of clustering.\n",
    "\n",
    "During your study, you discovered seven findings, which are listed in the data points below. Using the K-means algorithm, you want to build three clusters from these observations. The clusters C1, C2, and C3 have the following findings after the first iteration:\n",
    "C1: (2,2), (4,4), (6,6);\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4);\n",
    "C3: (5,5) and (9,9)\n",
    "\n",
    "What would the cluster centroids be if you were to run a second iteration? What would this clustering's SSE be?\n",
    "\n",
    "Second Iteration:\n",
    "\n",
    "C1 centroid: (4, 4)\n",
    "C2 centroid: (0, 4)\n",
    "C3 centroid: (7, 7)\n",
    "SSE calculation:\n",
    "\n",
    "SSE = Sum of squared distances of each data point to its assigned cluster centroid.\n",
    "SSE = (2-4)^2 + (4-4)^2 + (6-4)^2 + (0-0)^2 + (4-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (5-7)^2 + (9-7)^2\n",
    "= 4 + 0 + 4 + 0 + 16 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 4 + 4\n",
    "= 36\n",
    "\n",
    "10. In a software project, the team is attempting to determine if software flaws discovered duringtesting are identical. Based on the text analytics of the defect details, they decided to build 5 clustersof related defects. Any new defect formed after the 5 clusters of defects have been identified mustbe listed as one of the forms identified by clustering. A simple diagram can beused to explain thisprocess. Assume you have 20 defect data points that are clustered into 5 clusters and you used the k-means algorithm.\n",
    "\n",
    "The k-means algorithm is a popular choice for clustering in this scenario. Let's assume that the 20 defect data points have been successfully clustered into 5 clusters using the k-means algorithm. Each cluster represents a distinct form of software flaw.\n",
    "\n",
    "Here's a simple diagram to illustrate the clustering process:\n",
    "\n",
    "Cluster 1:    Cluster 2:    Cluster 3:    Cluster 4:    Cluster 5:\n",
    "   Defect A       Defect E       Defect I        Defect M        Defect Q\n",
    "   Defect B       Defect F       Defect J        Defect N        Defect R\n",
    "   Defect C       Defect G       Defect K        Defect O        Defect S\n",
    "   Defect D       Defect H       Defect L        Defect P        Defect T\n",
    "   \n",
    "In this diagram, each defect is represented by a letter (A, B, C, etc.), and the clusters are organized horizontally. Each cluster contains the defects that are similar based on the text analytics of the defect details.\n",
    "\n",
    "Once the initial clustering is done, any new defect that is discovered can be assigned to one of the existing clusters based on its similarity to the defects already identified. If the new defect closely resembles the characteristics of an existing cluster, it can be added to that cluster. This way, the team can maintain the identified forms of software flaws and ensure that any new defects are properly categorized.\n",
    "\n",
    "The k-means algorithm allows for easy assignment of new data points to existing clusters based on the distance from the cluster centroids. By calculating the distance of the new defect from the centroids of the existing clusters, the team can determine which cluster it belongs to and update the cluster accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86771dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861104d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547dacf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6983bb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939828f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c858964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f338e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76048799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d9e76b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
